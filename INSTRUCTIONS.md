- **Project (35 marks) to be done in pair**: Time-series data: [gurgaon.csv](http://www.cse.iitd.ac.in/~rijurekha/col851/df_ggn_covariates.csv), and [patna.csv](http://www.cse.iitd.ac.in/~rijurekha/col851/df_patna_covariates.csv). Column 5 is calibrated Particulate Matter (PM 2.5) values downloaded from [https://vayu.undp.org.in/](https://vayu.undp.org.in/%22), averaged over all sensors in the city over an hour, column 1 is hourly timestamp (the data is for 185 days between Jul-Dec 2024), columns 2, 3 and 4 are meteorological factors relative humidity, temperature and wind speed respectively, which possibly affect PM.  
      
    Parts 1 and 2 demos on Oct 13.  
    Parts 3, 4,and 5 demos on Nov 27, 28.  
      
- **Part 1: Analyzing the data and the pre-trained forecasting model** Run the pre-trained time-series LLM model for forecasting from Amazon [chronos](https://github.com/amazon-science/chronos-forecasting), on the data of the two cities above for PM forecasting task, on your laptop CPU. Use only column 1 and column 5 for this part, ignoring the meteorologival factors. Experiment in zero-shot mode (i.e. without any fine-tuning), with (i) diferent context lengths (i.e. how many days in the past is used in forecasting the future),(ii) different forecasting horizons (i.e. how much into the future you forecast) and (iii) different pre-trained model variants, based on the compute power of your laptops. For 24 hours horizon, plot average RMSE with different context lengths (2 days, 4 days, 8 days, 10 days, 14 days) for the biggest model-variant you can run. For 10 days context length, plot average RMSE with different horizons (4 hours, 8 hours, 12 hours, 24 hours, 48 hours). Add the plots in your report. Discuss which model variant, context length and horizon gives best average RMSE for the two cities. **\[6 marks]**
- **Part 2: Analyzing performance metrics** Build a measurement framework, e.g. using perftool, to visualize forecasting inference performance for part 1 on laptop CPU, including performance metrics like inference latency, throughput, CPU utilization, DRAM utilization, cache hits/misses, etc.. For a loop of continuously running forecasting inferences, show these performance metrics (e.g. using grafana) in real time. Also plot the metrics varying context length, forecasting window and model variant, and add the plots in your report. Discuss RMSE (measured in part 1) vs. performance (measured in part 2) trade-offs if any. **\[4 marks]**
- **Part 3: Does adding meteorological factors improve forecasting RMSE?** For the best set of hyper-parameters found in part 1 (model variant, context length, horizon), plot the CDF of RMSE to see the whole distribution, instead of just the average RMSE. Plot test day along x-axis, ground truth and forecasted PM, and RMSE along y-axis, and discuss if high RMSE on some test-days is related to sudden increase or decrease in PM, causing difficulty in forecasting. Now try and include the meteorological [covariates](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html#incorporating-the-covariates), and see if RMSE improves. If you need forecasted values of the covariates to forecast PM, use ground truth values of the covariates from the csv (assuming we have perfect forecast for weather parameters). For covariate inclusion, dataset might have to be divided into train-test set, as catboost/xgboost for mapping covariates to PM will need training. Use 80-20 split, and make sure to compare RMSE with and without covariates on the same 20% test data, for apple-to-apple comparison. You can also use [Chronos-2](https://huggingface.co/amazon/chronos-2) for this covariate analysis. This already has support to add more time-series variables, so instead of train-test with 80-20, sero-shot forecasting can be done. Plot performance metrics with covariates, and compare with the performance metrics measured without covariates, in part 2 (make sure you use the same model variant with and without co-variates). **\[7 marks]**
- **Part 4: Checking performance on more constrained embedded hardware** Run forecasting with and without covariates on Raspberry PI and compare performance metrics w.r.t your laptop measurements. Pin the process to 1, 2, 3 ... cores of the PI, and see the effect of reducing CPU cores. Additionally measure CPU temperature every minute, running inferences continuouly for 30 mins. Include these plots in your report, with discussion. This is relevant for personal PM exposure apps, that might be available on Android phones.**\[8 marks]**
- **Part 5: Implementing an inference server** Implement a server-client program on your laptop, so that the client sends forecasting requests with different hyper-parameters (context lengths, horizons, model variant, with and without co-variate) to the server. The server runs the appropriate request and returns the forecasted PM value to the client. List all LLM optimizations covered in lectures, with paper references. Mention which optimizations are not applicable on the CPU server, which are applicable but cannot be implmented due to restrictions with pre-trained models, compilers etc., and try and implement the rest (you can add more optimizations, which were not discussed in class, but makes sense to you). Create a trace of 500 client requests, using the datasets. Measure performance metrics for the server to handle these 500 requests, where (a) client does not wait between sending two requests, (b) client genrates requests with Poisson distribution. Plot without any implemented optimization, and with your optimizations. Discuss the effect of each optimization on server performance. **\[10 marks]**